AIM: Implement Convolutional Neural Network for Digit Recognition on the MNIST Dataset.


CODE:
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Load the MNIST dataset
# MNIST dataset consists of handwritten digits (0-9)
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize pixel values to the range [0, 1]
# Pixel values range from 0 to 255; this scales them to 0 to 1
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape the images to add a channel dimension (required for CNN)
# CNNs expect input in the form (height, width, channels)
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# One-hot encode the target labels
# Convert labels to one-hot encoded format for categorical classification
y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)

# Define the CNN model
def create_cnn_model():
    model = Sequential([
        # First convolutional layer with 32 filters, 3x3 kernel, ReLU activation
        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        # First max pooling layer with 2x2 pool size
        MaxPooling2D((2, 2)),
        # Second convolutional layer with 64 filters, 3x3 kernel, ReLU activation
        Conv2D(64, (3, 3), activation='relu'),
        # Second max pooling layer with 2x2 pool size
        MaxPooling2D((2, 2)),
        # Third convolutional layer with 64 filters, 3x3 kernel, ReLU activation
        Conv2D(64, (3, 3), activation='relu'),
        # Flatten the output from 2D to 1D for the dense layer
        Flatten(),
        # Fully connected layer with 64 neurons and ReLU activation
        Dense(64, activation='relu'),
        # Output layer with 10 neurons (one for each class) and softmax activation
        Dense(10, activation='softmax')
    ])
    return model

# Function to train and evaluate the CNN model
def train_and_evaluate_cnn_model():
    model = create_cnn_model()  # Create the CNN model
    # Compile the model with Adam optimizer and categorical cross-entropy loss
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    # Train the model with the training data, use validation data for evaluation
    model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1, validation_data=(X_test, y_test))
    return model

# Train and evaluate the CNN model
cnn_model = train_and_evaluate_cnn_model()










































































#CODE EXPLANATION:
#Explanation:
#Import Libraries:

#Import necessary libraries for building and training the model.
#Load Data:

#Load the MNIST dataset which includes training and testing images of handwritten digits.
#X_train and X_test contain the image data.
#y_train and y_test contain the labels for the images.
#Normalize Data:

#Normalize the pixel values to be between 0 and 1 by dividing by 255.
#Reshape Data:

#Reshape the images to add an extra dimension for the color channel (grayscale has one channel).
#One-hot Encode Labels:

#Convert the labels to one-hot encoded format, which is required for categorical classification.
#Define CNN Model:

#Create a Convolutional Neural Network (CNN) model with several layers:
#Conv2D layers to learn image features using convolutional filters.
#MaxPooling2D layers to reduce the spatial dimensions of the feature maps.
#Flatten layer to convert the 2D feature maps into 1D feature vectors.
#Dense layers to perform classification based on the learned features.
#Softmax activation in the final layer to output class probabilities.
#Train and Evaluate Model:

#Compile the model using the Adam optimizer and categorical cross-entropy loss.
#Train the model with the training data, using some of the test data for validation.
#The model is trained for 5 epochs with a batch size of 64.
#This code builds, trains, and evaluates a CNN on the MNIST dataset for the task of handwritten digit classification.










#OUTPUT EXPLANATION:
#The output of the above code consists of several stages, including the training process and the final evaluation of the CNN model. Here's what #you can expect to see and understand from the output:

#Training Process Output
#During the training process, the model will output information for each epoch, which includes:

#Epoch Information:

#The training progress is shown for each epoch (iteration over the entire training dataset).
#Example output for an epoch: Epoch 1/5
#Loss and Accuracy:

#loss: The loss value on the training dataset after each epoch. Lower loss indicates a better fit to the training data.
#accuracy: The accuracy on the training dataset after each epoch.
#Example: loss: 0.0234 - accuracy: 0.9916
#Validation Loss and Accuracy:

#val_loss: The loss value on the validation dataset (test data) after each epoch. This helps in understanding how well the model is #generalizing.
#val_accuracy: The accuracy on the validation dataset after each epoch.
#Example: val_loss: 0.0457 - val_accuracy: 0.9853
#The output will look similar to this for each epoch:

#arduino
#Copy code
#Epoch 1/5
#938/938 [==============================] - 14s 15ms/step - loss: 0.0234 - accuracy: 0.9916 - val_loss: 0.0457 - val_accuracy: 0.9853
#Epoch 2/5
#938/938 [==============================] - 13s 14ms/step - loss: 0.0162 - accuracy: 0.9947 - val_loss: 0.0412 - val_accuracy: 0.9867
#...
#Epoch 5/5
#938/938 [==============================] - 14s 15ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.0397 - val_accuracy: 0.9872
#Evaluation Output
#After training, the model will be evaluated on the test dataset, and you will get the following metrics:

#Final Test Loss: The loss value on the test dataset.
#Final Test Accuracy: The accuracy of the model on the test dataset.
#Example output:

#yaml
#Copy code
#Test loss: 0.0397
#Test accuracy: 0.9872
#Visualization (Not Included in Current Code)
#Although the provided code does not include visualization, you can add it to better understand the model's performance. For example, you can #plot some test images along with their predicted labels.

#Interpretation
#Training Accuracy and Loss: High accuracy and low loss on the training set indicate that the model is learning well from the training data.
#Validation Accuracy and Loss: These metrics help you understand how well the model generalizes to unseen data. Ideally, they should be close to #the training accuracy and loss.
#Overfitting: If the training accuracy is much higher than the validation accuracy, it might indicate overfitting. The model is performing well #on training data but not on validation data.
#Test Accuracy and Loss: These final metrics indicate the model's performance on completely unseen data. A high test accuracy means the model is #good at recognizing handwritten digits from the MNIST dataset.
#By analyzing these outputs, you can gauge the effectiveness of your CNN model and understand areas for potential improvement.







#Background Information
#Convolutional Neural Network (CNN):

#A type of deep learning model specifically designed for working with grid-like data such as images.
#CNNs are highly effective for image recognition tasks because they can automatically learn to detect important features such as edges, #textures, and shapes.
#MNIST Dataset:

#A well-known dataset in the machine learning community that contains 70,000 images of handwritten digits (0-9).
#Each image is 28x28 pixels, and the dataset is split into 60,000 training images and 10,000 testing images.
#Steps to Implement CNN for Digit Recognition
#Step 1: Import Necessary Libraries

#Step 2: Load and Prepare the Data
#Load the MNIST dataset.
#Normalize the pixel values to the range [0, 1].
#Reshape the images to add a channel dimension (required for CNN).
#One-hot encode the target labels.

#Step 3: Define the CNN Model
#Conv2D layers to learn features from the images.
#MaxPooling2D layers to reduce the spatial dimensions of the feature maps.
#Flatten layer to convert 2D feature maps into a 1D vector.
#Dense layers for classification.


#Step 4: Train and Evaluate the CNN Model
#Compile the model with an optimizer and loss function.
#Train the model using the training data.
#Evaluate the model on the test data

#Explanation:
#Import Libraries: Load the necessary libraries for building and training the model.
#Load Data: Load and preprocess the MNIST dataset.
#Normalize the pixel values to be between 0 and 1.
#Reshape the images to include a channel dimension for CNN compatibility.
#Convert the labels to one-hot encoded format for categorical classification.
#Build Model: Create a CNN model with several convolutional and pooling layers to learn features from the images, followed by fully connected #layers for classification.
#Train Model: Compile and train the model using the training data and validate it using the test data.
#Evaluate Model: The model's performance is measured by its accuracy and loss on the test dataset.

#Output
#Training Process: For each epoch, you'll see the loss and accuracy on the training data, and the loss and accuracy on the validation data (test #set).
#Final Metrics: After training, the final loss and accuracy on the test dataset will be displayed, indicating how well the model performs on #unseen data.
#By following these steps, you can build, train, and evaluate a Convolutional Neural Network for digit recognition using the MNIST dataset.
